#!/usr/bin/env python3
"""
Standalone Q&A System with RAG and Llama 3.1

This module provides a simple interface for answering questions using
retrieval-augmented generation with Llama 3.1 model.
"""

import sys
import os
import json
from typing import List, Dict, Any
import requests
from retriever import EmbeddingRetriever

class QASystem:
    def __init__(self, ollama_url: str = "http://localhost:11434", model: str = "llama3.1"):
        """
        Initialize the Q&A system.
        
        Args:
            ollama_url: URL of the Ollama server
            model: Name of the model to use
        """
        self.ollama_url = ollama_url
        self.model = model
        self.retriever = EmbeddingRetriever()
        
    def create_professional_prompt(self, question: str, context: List[Dict[str, Any]]) -> str:
        """
        Create a professional prompt for Llama 3.1 with retrieved context.
        
        Args:
            question: User's question
            context: Retrieved document chunks with metadata
            
        Returns:
            Formatted prompt string
        """
        # Format context documents
        context_text = ""
        for i, doc in enumerate(context, 1):
            context_text += f"\n[Document {i}]\n"
            context_text += f"Content: {doc['content']}\n"
            if 'source' in doc:
                context_text += f"Source: {doc['source']}\n"
            context_text += "-" * 50 + "\n"
        
        # Detect question language
        question_lower = question.lower()
        french_indicators = ['quoi', 'que', 'comment', 'pourquoi', 'oÃ¹', 'quand', 'qui', 'quel', 'quelle', 
                           "qu'est-ce", "qu'est", 'est-ce', 'une', 'des', 'les', 'dans', 'avec', 'pour', 
                           'assurance', 'prime', 'contrat', 'garantie']
        is_french = any(word in question_lower for word in french_indicators)
        is_arabic = any(char in question for char in 'Ø£Ø¥Ø¢Ø©ØªØ«Ø¬Ø­Ø®Ø¯Ø°Ø±Ø²Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚ÙƒÙ„Ù…Ù†Ù‡ÙˆÙ‰ÙŠ')
        
        if is_arabic:
            prompt = f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„ØªØ£Ù…ÙŠÙ† Ù„Ø´Ø±ÙƒØ© BH Ù„Ù„ØªØ£Ù…ÙŠÙ†. ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…Ø®ØªØµØ±Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

**Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:**
1. Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø£Ø¯Ù†Ø§Ù‡ ÙÙ‚Ø·
2. Ø¥Ø°Ø§ Ù„Ù… ØªØ­ØªÙˆÙŠ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ©ØŒ Ø§Ø°ÙƒØ± Ø°Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­
3. Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…Ø®ØªØµØ±Ø© (Ù„Ø§ ØªØ²ÙŠØ¯ Ø¹Ù† 3-4 Ø¬Ù…Ù„)
4. Ø§Ø°ÙƒØ± Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø¹Ù†Ø¯ Ø§Ù„Ø¶Ø±ÙˆØ±Ø©
5. Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø·Ø§Ø¨Ø¹ Ù…Ù‡Ù†ÙŠ ÙˆÙ…ÙÙŠØ¯

**ÙˆØ«Ø§Ø¦Ù‚ Ø§Ù„Ø³ÙŠØ§Ù‚:**{context_text}

**Ø§Ù„Ø³Ø¤Ø§Ù„:** {question}

**Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:**"""
        elif is_french:
            prompt = f"""Vous Ãªtes un assistant IA spÃ©cialisÃ© dans l'assurance pour BH Assurance. Vous devez fournir des rÃ©ponses prÃ©cises et concises en franÃ§ais.

**INSTRUCTIONS:**
1. RÃ©pondez Ã  la question en utilisant UNIQUEMENT les informations fournies dans les documents ci-dessous
2. Si les documents ne contiennent pas suffisamment d'informations, indiquez-le clairement
3. Fournissez une rÃ©ponse prÃ©cise et concise (maximum 3-4 phrases)
4. Citez les sources si nÃ©cessaire
5. Maintenez un ton professionnel et utile

**DOCUMENTS DE CONTEXTE:**{context_text}

**QUESTION:** {question}

**RÃ‰PONSE:**"""
        else:
            prompt = f"""You are a professional AI assistant specialized in insurance for BH Insurance. You must provide accurate and concise responses.

**INSTRUCTIONS:**
1. Answer the question using ONLY the information provided in the context documents below
2. If the context doesn't contain enough information, clearly state this limitation
3. Provide a precise and concise response (maximum 3-4 sentences)
4. Cite sources when necessary
5. Maintain a professional and helpful tone

**CONTEXT DOCUMENTS:**{context_text}

**QUESTION:** {question}

**ANSWER:**"""
        
        return prompt
    
    def call_llama(self, prompt: str) -> str:
        """
        Call Llama 3.1 model via Ollama API.
        
        Args:
            prompt: The formatted prompt
            
        Returns:
            Generated response from the model
        """
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": 2048,
                        "stop": ["\n\n**QUESTION:**", "\n\n**CONTEXT DOCUMENTS:**"]
                    }
                },
                timeout=120
            )
            
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '').strip()
            else:
                return f"Error: Failed to get response from model (Status: {response.status_code})"
                
        except requests.exceptions.RequestException as e:
            return f"Error: Failed to connect to Ollama server - {str(e)}"
        except Exception as e:
            return f"Error: Unexpected error - {str(e)}"
    
    def answer_question(self, question: str, k: int = 5) -> Dict[str, Any]:
        """
        Answer a question using RAG approach.
        
        Args:
            question: User's question
            k: Number of documents to retrieve
            
        Returns:
            Dictionary containing answer, sources, and metadata
        """
        try:
            # Retrieve relevant documents
            print(f"ğŸ” Searching for relevant information...")
            search_results = self.retriever.search(question, top_k=k)
            
            if not search_results:
                return {
                    "answer": "I couldn't find any relevant information in the knowledge base to answer your question.",
                    "sources": [],
                    "metadata": {
                        "retrieved_docs": 0,
                        "model": self.model,
                        "status": "no_results"
                    }
                }
            
            print(f"ğŸ“š Found {len(search_results)} relevant documents")
            
            # Convert SearchResult objects to dictionaries for prompt creation
            context_docs = []
            for result in search_results:
                context_docs.append({
                    'content': result.text,
                    'source': result.filename,
                    'score': result.score
                })
            
            # Create professional prompt
            prompt = self.create_professional_prompt(question, context_docs)
            
            # Generate answer using Llama 3.1
            print(f"ğŸ¤– Generating answer with {self.model}...")
            answer = self.call_llama(prompt)
            
            # Extract sources
            sources = []
            for result in search_results:
                source_info = {
                    "content": result.text[:200] + "..." if len(result.text) > 200 else result.text,
                    "score": result.score,
                    "source": result.filename
                }
                sources.append(source_info)
            
            return {
                "answer": answer,
                "sources": sources,
                "metadata": {
                    "retrieved_docs": len(search_results),
                    "model": self.model,
                    "status": "success"
                }
            }
            
        except Exception as e:
            return {
                "answer": f"An error occurred while processing your question: {str(e)}",
                "sources": [],
                "metadata": {
                    "retrieved_docs": 0,
                    "model": self.model,
                    "status": "error",
                    "error": str(e)
                }
            }

def main():
    """
    Main function for command-line usage.
    """
    print("ğŸš€ Q&A System with RAG and Llama 3.1")
    print("=" * 40)
    
    # Initialize Q&A system
    qa_system = QASystem()
    
    if len(sys.argv) > 1:
        # Question provided as command line argument
        question = " ".join(sys.argv[1:])
        print(f"\nâ“ Question: {question}\n")
        
        result = qa_system.answer_question(question)
        
        print("\nğŸ’¡ Answer:")
        print("-" * 20)
        print(result['answer'])
        
        if result['sources']:
            print("\nğŸ“– Sources:")
            print("-" * 20)
            for i, source in enumerate(result['sources'], 1):
                print(f"{i}. {source['content']} (Score: {source['score']:.3f})")
                if 'source' in source:
                    print(f"   Source: {source['source']}")
                print()
        
        print(f"\nğŸ“Š Metadata: {json.dumps(result['metadata'], indent=2)}")
        
    else:
        # Interactive mode
        print("\nEntering interactive mode. Type 'quit' to exit.\n")
        
        while True:
            try:
                question = input("â“ Enter your question: ").strip()
                
                if question.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ Goodbye!")
                    break
                
                if not question:
                    continue
                
                print()
                result = qa_system.answer_question(question)
                
                print("\nğŸ’¡ Answer:")
                print("-" * 20)
                print(result['answer'])
                
                if result['sources']:
                    print("\nğŸ“– Sources:")
                    print("-" * 20)
                    for i, source in enumerate(result['sources'], 1):
                        print(f"{i}. {source['content']} (Score: {source['score']:.3f})")
                        if 'source' in source:
                            print(f"   Source: {source['source']}")
                        print()
                
                print("-" * 50)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ Goodbye!")
                break
            except Exception as e:
                print(f"\nâŒ Error: {e}")
                continue

if __name__ == "__main__":
    main()